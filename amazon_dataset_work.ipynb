{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"amazon_dataset_work.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Lt3c4mDmH3Gu"},"source":["Our team decided to stop working with the amazon dataset because of problems accessing cohesive identification for each book using ASIN labels. First, we tried to read the csv file of all 51 million samples on Google Colab. Since this file is so big, it caused the notebook to run out of memory before the whole file was read."]},{"cell_type":"code","metadata":{"id":"ZlB6NrtDHzo6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618204095092,"user_tz":420,"elapsed":4521,"user":{"displayName":"Anna Riley","photoUrl":"","userId":"13790780312400946219"}},"outputId":"fc24f5d8-0a6b-441b-eed7-fea7bd2ce022"},"source":["import pandas as pd\n","import pandas as pd\n","!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/all_csv_files.csv \n","df = pd.read_csv('all_csv_files.csv')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2021-04-12 05:08:11--  http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/all_csv_files.csv\n","Resolving deepyeti.ucsd.edu (deepyeti.ucsd.edu)... 169.228.63.50\n","Connecting to deepyeti.ucsd.edu (deepyeti.ucsd.edu)|169.228.63.50|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 9726085576 (9.1G) [application/octet-stream]\n","Saving to: ‘all_csv_files.csv’\n","\n","all_csv_files.csv     0%[                    ]  64.71M  31.1MB/s               ^C\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nzhiSdYReEqb"},"source":["So, I downloaded the csv file to my local computer and ran the below code to split the file into 8 csv file chunks that are small enough to load into google colab."]},{"cell_type":"code","metadata":{"id":"ir86wQjD7t_E","colab":{"base_uri":"https://localhost:8080/","height":442},"executionInfo":{"status":"error","timestamp":1618197314845,"user_tz":420,"elapsed":358,"user":{"displayName":"Ritchie Xia","photoUrl":"https://lh4.googleusercontent.com/-9b17yyHa-UM/AAAAAAAAAAI/AAAAAAAAAdA/KA6kEg3fbUg/s64/photo.jpg","userId":"06486437088501598520"}},"outputId":"1e778011-eab5-4089-d638-63f9bcc89f4e"},"source":["df= pd.read_csv(\"all_csv_files.csv\")\n","print(df)\n","for i in range(8):\n","\t(df[i * 25000000:(i + 1) * 25000000]).to_csv(str(i), index=True)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-1a0d6874586c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all_csv_files.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m25000000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m25000000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'all_csv_files.csv'"]}]},{"cell_type":"markdown","metadata":{"id":"sIQoJY-ze75X"},"source":["Then after doing this I was able to see the data contents. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"QmxGnOilerF0","executionInfo":{"status":"ok","timestamp":1618204098180,"user_tz":420,"elapsed":710,"user":{"displayName":"Anna Riley","photoUrl":"","userId":"13790780312400946219"}},"outputId":"03378edb-dc01-46a7-9b60-db66083e54f6"},"source":["df=pd.read_csv('/content/asindata_0')\n","df"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>B00ZGTMHVU</th>\n","      <th>A1HGH7NF01K3W7</th>\n","      <th>5.0</th>\n","      <th>1451001600</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>B013CG1EVC</td>\n","      <td>A3GDQGSD4C0HOB</td>\n","      <td>5.0</td>\n","      <td>1.496966e+09</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>B013CG1EVC</td>\n","      <td>A1C88HIIEVN0QN</td>\n","      <td>5.0</td>\n","      <td>1.445904e+09</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>B013CG1EVC</td>\n","      <td>A1G5MTDE3A5LSL</td>\n","      <td>5.0</td>\n","      <td>1.440893e+09</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>B013CG1XS6</td>\n","      <td>A3SOHSDN51M5NQ</td>\n","      <td>5.0</td>\n","      <td>1.441498e+09</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>B0096E8EPY</td>\n","      <td>AYRFEPLLS7A7O</td>\n","      <td>3.0</td>\n","      <td>1.368403e+09</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>346505</th>\n","      <td>346505</td>\n","      <td>B00UCIXNJK</td>\n","      <td>A3ASO1AU1B77NN</td>\n","      <td>5.0</td>\n","      <td>1.440893e+09</td>\n","    </tr>\n","    <tr>\n","      <th>346506</th>\n","      <td>346506</td>\n","      <td>B00UCIXNJK</td>\n","      <td>A2GUO7Y2MCMH84</td>\n","      <td>5.0</td>\n","      <td>1.439078e+09</td>\n","    </tr>\n","    <tr>\n","      <th>346507</th>\n","      <td>346507</td>\n","      <td>B00UCIXNJK</td>\n","      <td>A2KI8J2BSSK7YY</td>\n","      <td>5.0</td>\n","      <td>1.430957e+09</td>\n","    </tr>\n","    <tr>\n","      <th>346508</th>\n","      <td>346508</td>\n","      <td>B00UCIXNJK</td>\n","      <td>A1EZ9TNVQFLT8C</td>\n","      <td>5.0</td>\n","      <td>1.430870e+09</td>\n","    </tr>\n","    <tr>\n","      <th>346509</th>\n","      <td>346509</td>\n","      <td>B00UCIXNJK</td>\n","      <td>A1VET4OZ5S9</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>346510 rows × 5 columns</p>\n","</div>"],"text/plain":["        Unnamed: 0  B00ZGTMHVU  A1HGH7NF01K3W7  5.0    1451001600\n","0                0  B013CG1EVC  A3GDQGSD4C0HOB  5.0  1.496966e+09\n","1                1  B013CG1EVC  A1C88HIIEVN0QN  5.0  1.445904e+09\n","2                2  B013CG1EVC  A1G5MTDE3A5LSL  5.0  1.440893e+09\n","3                3  B013CG1XS6  A3SOHSDN51M5NQ  5.0  1.441498e+09\n","4                4  B0096E8EPY   AYRFEPLLS7A7O  3.0  1.368403e+09\n","...            ...         ...             ...  ...           ...\n","346505      346505  B00UCIXNJK  A3ASO1AU1B77NN  5.0  1.440893e+09\n","346506      346506  B00UCIXNJK  A2GUO7Y2MCMH84  5.0  1.439078e+09\n","346507      346507  B00UCIXNJK  A2KI8J2BSSK7YY  5.0  1.430957e+09\n","346508      346508  B00UCIXNJK  A1EZ9TNVQFLT8C  5.0  1.430870e+09\n","346509      346509  B00UCIXNJK     A1VET4OZ5S9  NaN           NaN\n","\n","[346510 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"dBQ5hqlufh7O"},"source":["We needed to find a way to use the ASIN label of each book to find either the ISBN number or the book title. The below code is our attempt to do this. We found the url of the book on Amazon using the ASIN number with leading zeros, and used requests and beautifulsoup to parse the HTML. We realized that Amazon does not let people scrap data from their website in this way, and the only way you can automatically convert ASIN labels to ISBN labels is if you have a vendor account. So, we make the decision to stop working on this dataset. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zg8i02ycEJzo","executionInfo":{"status":"ok","timestamp":1618197362010,"user_tz":420,"elapsed":8336,"user":{"displayName":"Ritchie Xia","photoUrl":"https://lh4.googleusercontent.com/-9b17yyHa-UM/AAAAAAAAAAI/AAAAAAAAAdA/KA6kEg3fbUg/s64/photo.jpg","userId":"06486437088501598520"}},"outputId":"c2f9f035-fdc5-4bb3-b766-6a1df1fa8461"},"source":["# extracting elements from page for Amazon dataset -- in the end the Amazon dataset was determined to be too big to use\n","\n","import torch, torchtext, numpy as np\n","import pandas as pd, csv\n","from torch import nn, optim\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","import pdb\n","torch.manual_seed(291)\n","np.random.seed(291)\n","import pandas as pd\n","import requests\n","import gzip\n","!pip install bs4\n","from bs4 import BeautifulSoup\n","\n","HEADERS = ({'User-Agent':\n","            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n","            'Accept-Language': 'en-US, en;q=0.5'})   # need to be user-agent to scrape info from page\n","URL = \"https://www.amazon.com/dp/0001061240\"    # random book to test the asin extension\n","webpage = requests.get(URL, headers=HEADERS)\n","soup = BeautifulSoup(webpage.content, \"lxml\")\n","\n","title = soup.find(\"span\", attrs={\"id\":'productTitle'})  # extract specific element from page\n","title_value = title.string # convert to string\n","title_string = title_value.strip() # get rid of empty space\n","title_string\n","\n","def getURL(asin):\n","  filler_length = 10 - len(asin)\n","  filler = \"\"\n","  for x in range(filler_length):\n","    filler += str(0)\n","  filler += str(asin)\n","  filler = \"https://www.amazon.com/dp/\" + filler\n","  return filler"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"pxhHcLOHErrB","executionInfo":{"status":"ok","timestamp":1618197366208,"user_tz":420,"elapsed":1910,"user":{"displayName":"Ritchie Xia","photoUrl":"https://lh4.googleusercontent.com/-9b17yyHa-UM/AAAAAAAAAAI/AAAAAAAAAdA/KA6kEg3fbUg/s64/photo.jpg","userId":"06486437088501598520"}},"outputId":"f79a455b-ac82-420e-aa01-d991f35a4cea"},"source":["URL = getURL(\"1061240\")\n","webpage = requests.get(URL, headers=HEADERS)\n","soup = BeautifulSoup(webpage.content, \"lxml\")\n","title = soup.find(\"span\", attrs={\"id\":'productTitle'})  # extract specific element from page\n","title_value = title.string # convert to string\n","title_string = title_value.strip() # get rid of empty space\n","title_string"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'The golden treasury of poetry'"]},"metadata":{"tags":[]},"execution_count":4}]}]}